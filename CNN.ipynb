{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7kVpQjs_syk",
        "outputId": "640a1b2a-3058-46fa-935b-5cdd75927c79"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUhCRin2BD1_"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sub135XBGUw"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2hFLisUBIZj"
      },
      "source": [
        "df = pd.read_csv(\"new_data.csv\",index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "1IvGcxq2LM-0",
        "outputId": "46d11b69-3931-4d92-94c0-6572de689f0c"
      },
      "source": [
        "df.loc[1000,\"content\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i 've lost all hope day to day for the past year i have been thinking of suicide but never really started considering it a an option until recently i do n't know when or if i ever will kill myself but if my life keep heading in this direction then i will have no choice i sit here still procrastinating from overdue assignment that i would n't have done anyways i 've lived a lie for too long and i 'm tired of it i just want it to end now i hate feeling this way when i should be doing something with my life it 's only gon na get worse when i start highschool next year\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW-B2Ozcf1pB"
      },
      "source": [
        "df = pd.read_csv(\"new_data_nostopword.csv\",index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0RavijYncTy1",
        "outputId": "1793d678-cc36-428b-816f-e456441497c4"
      },
      "source": [
        "df[\"content\"][400]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"need way get going slowly working car mod game exam etc stopped feel hard get stuff done 'm procrastinating hard way get back\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZvDZqdx9jP"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeltCudrHafT"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jS9JtbDBQdr"
      },
      "source": [
        "train_df, test_df = train_test_split(df,test_size = 0.2, random_state=42,shuffle = True)\n",
        "train_df, valid_df = train_test_split(train_df,test_size=0.05, random_state=42,shuffle = True)\n",
        "train_df.to_csv(\"new_train.csv\",index = False)\n",
        "valid_df.to_csv(\"new_valid.csv\",index = False)\n",
        "test_df.to_csv(\"new_test.csv\",index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vOroRMxHmvM"
      },
      "source": [
        "X_train, X_valid, X_test = train_df[\"content\"], valid_df[\"content\"], test_df[\"content\"]\n",
        "y_train, y_valid, y_test = train_df[\"label\"], valid_df[\"label\"], test_df[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWlLDeNwLz3V"
      },
      "source": [
        "np.save(\"true_label_LR\",y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p1sTV0Izj3i"
      },
      "source": [
        "Data Engineered with One-Hot Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzNx_xUUzIBJ",
        "outputId": "c6e0454e-a008-4970-cd69-1ac64304f532"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "train_onehot = vectorizer.transform(X_train)\n",
        "test_onehot = vectorizer.transform(X_test)\n",
        "LR = LogisticRegression(max_iter = 5000,penalty='l2',C=1)\n",
        "LR.fit(train_onehot, y_train)\n",
        "y_pred_LR0 = LR.predict(test_onehot)\n",
        "print(\"Accuracy on test set is\",(y_pred_LR0 == y_test).sum()/len(y_test))\n",
        "np.save(\"y_pred_LR0\", y_pred_LR0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set is 0.8751733703190014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS9vz35DzpsE"
      },
      "source": [
        "Engineering with TF-IDF Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEp15hBoHuFY",
        "outputId": "083c66ca-ba3c-4a64-86b0-178c9f8df423"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "train_tfidf = vectorizer.transform(X_train)\n",
        "test_tfidf = vectorizer.transform(X_test)\n",
        "LR = LogisticRegression(max_iter = 5000,penalty='l2',C=1)\n",
        "LR.fit(train_tfidf, y_train)\n",
        "y_pred_LR1 = LR.predict(test_tfidf)\n",
        "print(\"Accuracy on test set is\",(y_pred_LR1 == y_test).sum()/len(y_test))\n",
        "np.save(\"y_pred_LR1\", y_pred_LR1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set is 0.8862690707350902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYs1rSc3CYHm"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzQWV5K-E6ku"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKWmBqyaGfR5"
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', batch_first=True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pw1bU4GiI-"
      },
      "source": [
        "fields = [('text',TEXT), ('label',LABEL)]\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = \"/content/drive/My Drive/\",\n",
        "                                        train = 'new_train.csv',\n",
        "                                        validation = \"new_valid.csv\",\n",
        "                                        test = 'new_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSSxkuhGu86"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = 'glove.6B.200d',\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owxVxqzsG0Lx"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5gLyhgyCa_w"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim), padding = (fs//2, 0)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        " \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zytQIb9NCppy"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [2, 3, 4]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPJgDwJVCwI7"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfXTMVZyEDwc"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    prediction_list = []\n",
        "    true_label= []\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "            prediction_list.append(rounded_preds.tolist())\n",
        "            true_label.append(batch.label.tolist())\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator),np.array([int(i) for batch in prediction_list for i in batch]), np.array([int(i) for batch in true_label for i in batch])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab3ZntyLD3sa",
        "outputId": "f5d0c562-8f63-4778-e837-9865295a483f"
      },
      "source": [
        "N_EPOCHS = 6\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc,_,_ = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.047 | Train Acc: 99.51%\n",
            "\t Val. Loss: 0.254 |  Val. Acc: 89.71%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.67%\n",
            "\t Val. Loss: 0.262 |  Val. Acc: 90.04%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.026 | Train Acc: 99.76%\n",
            "\t Val. Loss: 0.286 |  Val. Acc: 89.25%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.018 | Train Acc: 99.98%\n",
            "\t Val. Loss: 0.281 |  Val. Acc: 89.63%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.87%\n",
            "\t Val. Loss: 0.271 |  Val. Acc: 90.42%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.91%\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 90.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5BcfGX4HK2M",
        "outputId": "841f6550-26ba-4edf-da5d-d948957e5ad1"
      },
      "source": [
        "loss,acc, y_pred_CNN, true_label_CNN = evaluate(model, test_iterator,criterion)\n",
        "print(\"Wth CNN, accuracy on the test set is\", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wth CNN, accuracy on the test set is 0.9053333163261413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FvkGlfZIRly"
      },
      "source": [
        "np.save(\"y_pred_CNN\", y_pred_CNN)\n",
        "np.save(\"true_label_CNN\", true_label_CNN)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}